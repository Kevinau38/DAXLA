# THUáº¬T TOÃN CHI TIáº¾T - Há»† THá»NG PHÃT HIá»†N VI BIá»‚U Cáº¢M KHUÃ”N Máº¶T DAXLA

---

## ğŸ“Š Tá»”NG QUAN THUáº¬T TOÃN

### Pipeline Xá»­ LÃ½ ChÃ­nh
```
Input Video â†’ Face Detection â†’ Feature Extraction â†’ Classification â†’ Output Result
     â†“              â†“                â†“                  â†“              â†“
  Webcam      Haar Cascade      Pixel Features    Random Forest    Truth/Lie
```

---

## ğŸ¯ 1. THUáº¬T TOÃN PHÃT HIá»†N KHUÃ”N Máº¶T

### Haar Cascade Classifier

#### NguyÃªn LÃ½ Hoáº¡t Äá»™ng
```python
face_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")
faces = face_cascade.detectMultiScale(
    gray,                # áº¢nh grayscale
    scaleFactor=1.15,    # Tá»· lá»‡ thu nhá» má»—i láº§n quÃ©t
    minNeighbors=3,      # Sá»‘ lÆ°á»£ng neighbor tá»‘i thiá»ƒu
    minSize=(40, 40)     # KÃ­ch thÆ°á»›c khuÃ´n máº·t tá»‘i thiá»ƒu
)
```

#### Chi Tiáº¿t Thuáº­t ToÃ¡n
**BÆ°á»›c 1: Haar Features**
- Sá»­ dá»¥ng cÃ¡c pattern hÃ¬nh chá»¯ nháº­t Ä‘á»ƒ phÃ¡t hiá»‡n Ä‘áº·c trÆ°ng
- TÃ­nh toÃ¡n sá»± khÃ¡c biá»‡t cÆ°á»ng Ä‘á»™ sÃ¡ng giá»¯a cÃ¡c vÃ¹ng
- VÃ­ dá»¥: VÃ¹ng máº¯t thÆ°á»ng tá»‘i hÆ¡n vÃ¹ng mÃ¡

**BÆ°á»›c 2: Integral Image**
```
TÃ­nh toÃ¡n nhanh tá»•ng pixel trong hÃ¬nh chá»¯ nháº­t:
sum(x,y) = I(x,y) + sum(x-1,y) + sum(x,y-1) - sum(x-1,y-1)
```

**BÆ°á»›c 3: AdaBoost Learning**
- Káº¿t há»£p nhiá»u weak classifier thÃ nh strong classifier
- Chá»n cÃ¡c Haar features quan trá»ng nháº¥t
- Táº¡o cascade structure Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™

**BÆ°á»›c 4: Multi-scale Detection**
```python
# QuÃ©t áº£nh á»Ÿ nhiá»u kÃ­ch thÆ°á»›c khÃ¡c nhau
for scale in [1.0, 1.15, 1.32, ...]:
    resized_image = resize(image, scale)
    detect_faces(resized_image)
```

#### Tá»‘i Æ¯u HÃ³a
```python
# Chá»‰ láº¥y khuÃ´n máº·t lá»›n nháº¥t Ä‘á»ƒ trÃ¡nh false detection
largest_face = max(faces, key=lambda f: f[2] * f[3])
```

---

## ğŸ§  2. THUáº¬T TOÃN RANDOM FOREST

### Cáº¥u TrÃºc Tá»•ng Thá»ƒ

#### Hyperparameters
```python
RandomForestClassifier(
    n_estimators=50,      # 50 cÃ¢y quyáº¿t Ä‘á»‹nh
    max_depth=10,         # Äá»™ sÃ¢u tá»‘i Ä‘a 10 levels
    min_samples_split=10, # Tá»‘i thiá»ƒu 10 máº«u Ä‘á»ƒ split node
    min_samples_leaf=5,   # Tá»‘i thiá»ƒu 5 máº«u á»Ÿ leaf node
    random_state=42,      # Seed cho reproducibility
    n_jobs=-1            # Sá»­ dá»¥ng táº¥t cáº£ CPU cores
)
```

### Chi Tiáº¿t Thuáº­t ToÃ¡n

#### BÆ°á»›c 1: Bootstrap Sampling
```python
# Táº¡o n_estimators datasets con tá»« training data
for i in range(50):
    bootstrap_sample = random_sample_with_replacement(X_train, len(X_train))
    trees[i] = build_tree(bootstrap_sample)
```

#### BÆ°á»›c 2: Feature Randomness
```python
# Táº¡i má»—i node, chá»‰ xem xÃ©t sqrt(n_features) features ngáº«u nhiÃªn
n_features_per_split = int(sqrt(2304))  # sqrt(48*48) â‰ˆ 48 features
selected_features = random.choice(all_features, n_features_per_split)
```

#### BÆ°á»›c 3: Decision Tree Construction
```
Node Splitting Criteria:
â”œâ”€â”€ Gini Impurity: Gini = 1 - Î£(p_iÂ²)
â”œâ”€â”€ Information Gain: IG = H(parent) - Î£(w_i * H(child_i))
â””â”€â”€ Best Split: argmax(Information_Gain)

Stopping Conditions:
â”œâ”€â”€ max_depth = 10
â”œâ”€â”€ min_samples_split = 10
â”œâ”€â”€ min_samples_leaf = 5
â””â”€â”€ Pure node (Gini = 0)
```

#### BÆ°á»›c 4: Prediction Aggregation
```python
def predict(X):
    predictions = []
    for tree in trees:
        pred = tree.predict(X)
        predictions.append(pred)
    
    # Voting cho classification
    final_prediction = majority_vote(predictions)
    
    # Probability tá»« tá»· lá»‡ votes
    probability = count(predictions == final_prediction) / len(trees)
    
    return final_prediction, probability
```

### Æ¯u Äiá»ƒm Random Forest
1. **Giáº£m Overfitting**: Bootstrap + Feature randomness
2. **Robust**: Ãt nháº¡y cáº£m vá»›i noise vÃ  outliers
3. **Fast Inference**: Parallel prediction trÃªn nhiá»u trees
4. **Feature Importance**: ÄÃ¡nh giÃ¡ táº§m quan trá»ng cá»§a tá»«ng pixel

---

## ğŸ–¼ï¸ 3. THUáº¬T TOÃN Xá»¬ LÃ áº¢NH

### Data Preprocessing Pipeline

#### BÆ°á»›c 1: Image Loading & Conversion
```python
img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale
img_resized = cv2.resize(img, (48, 48))           # Resize to 48x48
```

#### BÆ°á»›c 2: Data Augmentation
```python
def augment_image(img):
    augmented = []
    
    # 1. Original image
    augmented.append(img)
    
    # 2. Horizontal flip (mirror effect)
    flipped = cv2.flip(img, 1)
    augmented.append(flipped)
    
    # 3. Rotation (5 degrees)
    rows, cols = img.shape
    M = cv2.getRotationMatrix2D((cols/2, rows/2), 5, 1)
    rotated = cv2.warpAffine(img, M, (cols, rows))
    augmented.append(rotated)
    
    return augmented  # 3x data increase
```

#### BÆ°á»›c 3: Feature Extraction
```python
# Flatten 2D image to 1D feature vector
feature_vector = img_resized.flatten()  # 48x48 = 2304 features

# Normalization
normalized_features = feature_vector.astype('float32') / 255.0
```

### Geometric Transformations

#### Rotation Matrix
```
R(Î¸) = [cos(Î¸)  -sin(Î¸)]
       [sin(Î¸)   cos(Î¸)]

For Î¸ = 5Â°:
R(5Â°) = [0.996  -0.087]
        [0.087   0.996]
```

#### Affine Transformation
```python
# Warp image using transformation matrix
cv2.warpAffine(src, M, (width, height))
# M: 2x3 transformation matrix
# Preserves parallel lines and ratios
```

---

## âš¡ 4. THUáº¬T TOÃN Tá»I Æ¯U HÃ“A REAL-TIME

### Frame Processing Optimization

#### Temporal Sampling
```python
# Xá»­ lÃ½ má»—i 2 frames Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
if self.frame_count % 2 == 0:
    process_frame(frame)
else:
    skip_frame()
```

#### Face Tracking
```python
# Cache tá»a Ä‘á»™ khuÃ´n máº·t Ä‘á»ƒ trÃ¡nh detect láº¡i
if faces_detected:
    self.last_face_coords = (x, y, w, h)
else:
    # Sá»­ dá»¥ng tá»a Ä‘á»™ cÅ© náº¿u khÃ´ng detect Ä‘Æ°á»£c
    use_cached_coordinates()
```

### Confidence Balancing Algorithm

#### Logic CÃ¢n Báº±ng
```python
def balance_prediction(prediction, probabilities):
    # Giáº£m false positive cho "Lie" class
    if prediction == 1 and probabilities[1] < 0.75:
        if probabilities[0] > 0.3:
            # Chuyá»ƒn vá» "Truth" náº¿u confidence khÃ´ng Ä‘á»§ cao
            prediction = 0
            confidence = probabilities[0]
    
    return prediction, confidence
```

#### Threshold Strategy
```
Confidence Thresholds:
â”œâ”€â”€ Minimum Detection: 0.55 (55%)
â”œâ”€â”€ Lie Confirmation: 0.75 (75%)
â”œâ”€â”€ Truth Fallback: 0.30 (30%)
â””â”€â”€ High Confidence: 0.85+ (85%+)
```

---

## ğŸ“ˆ 5. THUáº¬T TOÃN ÄÃNH GIÃ HIá»†U SUáº¤T

### Confusion Matrix Calculation

#### Metrics Computation
```python
# True/False Positives & Negatives
TP = sum((y_true == 1) & (y_pred == 1))  # Correctly predicted Lie
TN = sum((y_true == 0) & (y_pred == 0))  # Correctly predicted Truth
FP = sum((y_true == 0) & (y_pred == 1))  # False Lie detection
FN = sum((y_true == 1) & (y_pred == 0))  # Missed Lie detection

# Performance Metrics
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1_Score = 2 * (Precision * Recall) / (Precision + Recall)
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

#### Cross-Validation Strategy
```python
# Stratified split Ä‘á»ƒ cÃ¢n báº±ng classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y  # Äáº£m báº£o tá»· lá»‡ Truth/Lie giá»‘ng nhau
)
```

---

## ğŸ”„ 6. THUáº¬T TOÃN STREAMING & WEB

### Video Streaming Algorithm

#### MJPEG Streaming
```python
def generate_frames():
    while True:
        frame, results = detector.get_frame()
        
        # JPEG compression
        ret, jpeg = cv2.imencode('.jpg', frame, 
                                [cv2.IMWRITE_JPEG_QUALITY, 85])
        
        # HTTP multipart response
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + 
               jpeg.tobytes() + b'\r\n\r\n')
        
        time.sleep(0.033)  # ~30 FPS
```

#### Asynchronous Detection Updates
```javascript
// Client-side polling for results
setInterval(function() {
    $.getJSON('/detections', function(data) {
        updateUI(data);
    });
}, 500);  // Update every 500ms
```

### Memory Management
```python
# Efficient memory usage
frame_buffer = collections.deque(maxlen=5)  # Keep only 5 recent frames
result_cache = {}  # Cache recent predictions

# Garbage collection for long-running sessions
if frame_count % 1000 == 0:
    gc.collect()
```

---

## ğŸ¯ 7. THUáº¬T TOÃN PHÃ‚N LOáº I BIá»‚U Cáº¢M

### Emotion-to-Class Mapping

#### Binary Classification Strategy
```python
# Truth Class (Label 0)
truth_emotions = ['happy', 'neutral', 'surprise']
# Reasoning: Positive/neutral emotions indicate honesty

# Lie Class (Label 1)  
lie_emotions = ['angry', 'sad', 'fear', 'disgust']
# Reasoning: Negative emotions may indicate deception
```

#### Feature Space Analysis
```
48x48 Grayscale Image â†’ 2304-dimensional feature space

Key Facial Regions:
â”œâ”€â”€ Eyes: pixels [10:20, 15:35] â†’ Micro-expressions
â”œâ”€â”€ Mouth: pixels [25:35, 15:35] â†’ Smile/frown detection  
â”œâ”€â”€ Eyebrows: pixels [5:15, 10:40] â†’ Tension indicators
â””â”€â”€ Cheeks: pixels [15:30, 5:15, 35:45] â†’ Muscle movement
```

### Decision Boundary Optimization
```python
# Random Forest creates non-linear decision boundaries
# Each tree contributes to final decision surface
# Ensemble voting smooths decision boundaries
# Reduces overfitting to specific facial features
```

---

## ğŸš€ 8. THUáº¬T TOÃN DEPLOYMENT

### Model Serialization
```python
# Save trained model
with open('micro_model_simple.pkl', 'wb') as f:
    pickle.dump(model, f)

# Load for inference
with open('micro_model_simple.pkl', 'rb') as f:
    model = pickle.load(f)
```

### Production Optimizations
```python
# Model loading optimization
@lru_cache(maxsize=1)
def load_model():
    return pickle.load(open('micro_model_simple.pkl', 'rb'))

# Batch prediction for multiple faces
def batch_predict(face_regions):
    features = [preprocess(face) for face in face_regions]
    return model.predict_proba(np.array(features))
```

---

## ğŸ“Š 9. PHÃ‚N TÃCH COMPLEXITY

### Time Complexity
```
Face Detection: O(n * m * k)  # n=scales, m=positions, k=features
Feature Extraction: O(1)      # Fixed 48x48 â†’ 2304
Random Forest: O(log d * t)   # d=depth, t=trees
Total per frame: O(n * m * k + log d * t)
```

### Space Complexity
```
Model Storage: O(t * d * f)   # trees * depth * features
Runtime Memory: O(w * h * c)  # width * height * channels
Feature Vector: O(2304)       # Fixed size
```

### Performance Benchmarks
```
Typical Performance:
â”œâ”€â”€ Face Detection: ~10-15ms
â”œâ”€â”€ Feature Extraction: ~1-2ms  
â”œâ”€â”€ ML Prediction: ~2-3ms
â”œâ”€â”€ Total Latency: ~15-20ms
â””â”€â”€ Throughput: ~50-60 FPS
```

---

## ğŸ”§ 10. THUáº¬T TOÃN ERROR HANDLING

### Robust Prediction Pipeline
```python
def safe_predict(face_roi):
    try:
        # Preprocessing validation
        if face_roi is None or face_roi.size == 0:
            return None, 0.5
            
        # Size validation
        if min(face_roi.shape) < 20:
            return None, 0.5
            
        # Model prediction
        prediction, confidence = model_predict(face_roi)
        
        # Confidence validation
        if confidence < 0.55:
            return None, 0.5
            
        return prediction, confidence
        
    except Exception as e:
        logging.error(f"Prediction error: {e}")
        return None, 0.5
```

### Fallback Mechanisms
```python
# Graceful degradation
if not MODEL_AVAILABLE:
    return random_baseline_prediction()

if face_detection_fails():
    use_previous_face_coordinates()

if prediction_confidence_low():
    return_neutral_result()
```

---

## ğŸ¯ Káº¾T LUáº¬N THUáº¬T TOÃN

### Äiá»ƒm Máº¡nh
1. **Hiá»‡u Quáº£**: Random Forest cÃ¢n báº±ng tá»‘t giá»¯a accuracy vÃ  speed
2. **Robust**: Ãt bá»‹ overfitting nhá» ensemble method
3. **Real-time**: Tá»‘i Æ°u hÃ³a cho xá»­ lÃ½ video streaming
4. **Scalable**: Dá»… dÃ ng thÃªm features hoáº·c classes má»›i

### Äiá»ƒm Cáº§n Cáº£i Thiá»‡n
1. **Feature Engineering**: CÃ³ thá»ƒ sá»­ dá»¥ng deep features thay vÃ¬ raw pixels
2. **Temporal Modeling**: ThÃªm phÃ¢n tÃ­ch chuá»—i thá»i gian
3. **Multi-modal**: Káº¿t há»£p vá»›i audio hoáº·c physiological signals
4. **Personalization**: Adapt model cho tá»«ng ngÆ°á»i dÃ¹ng

### HÆ°á»›ng PhÃ¡t Triá»ƒn
1. **Deep Learning**: CNN/ResNet cho feature extraction
2. **Attention Mechanism**: Focus vÃ o vÃ¹ng quan trá»ng cá»§a khuÃ´n máº·t
3. **Sequence Modeling**: LSTM/Transformer cho temporal analysis
4. **Multi-task Learning**: Äá»“ng thá»i detect emotion vÃ  deception